# app/rag_chain.py
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  ÌïúÏñëÏúÑÌÇ§ Ï±óÎ¥á: VectorStore & Î™®Îç∏ Î°úÎî© Ï†ÑÏö©
#  (ÌîÑÎ°¨ÌîÑÌä∏¬∑QA Ï≤¥Ïù∏ÏùÄ routes.pyÏóêÏÑú Íµ¨ÏÑ±)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os, json
from dotenv import load_dotenv
from langchain.schema import Document
from langchain.text_splitter import TokenTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from tqdm import tqdm

# 1) ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú & Î™®Îç∏ Ï§ÄÎπÑ
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

embedding_model = OpenAIEmbeddings(
    openai_api_key=api_key,
    model="text-embedding-3-large", # ‚úÖ v3 ÎåÄÌòï Î™®Îç∏ Î™ÖÏãú
    dimensions=1024
)

llm = ChatOpenAI(
    openai_api_key=api_key,
    model_name="gpt-4o"  # ‚úÖ GPT-4oÎ°ú Î≥ÄÍ≤Ω
)

# 2) ÌÖçÏä§Ìä∏ Î∂ÑÌï†Í∏∞√ü
splitter = TokenTextSplitter(
    encoding_name="cl100k_base",
    chunk_size=800,
    chunk_overlap=100
)

# 3) Î¨∏ÏÑú Î°úÎçî
def load_documents(folder: str = "data/AllWikiPages") -> list[Document]:
    docs: list[Document] = []
    for root, _, files in os.walk(folder):
        for fname in files:
            if not fname.endswith(".json"):
                continue
            fpath = os.path.join(root, fname)
            try:
                with open(fpath, encoding="utf-8") as f:
                    data = json.load(f)
            except (json.JSONDecodeError, UnicodeDecodeError):
                continue

            text = json.dumps(data, ensure_ascii=False, separators=(",", ":"))
            rel   = os.path.relpath(fpath, folder).replace("\\", "/")
            label = os.path.splitext(fname)[0]

            parent = Document(page_content=text,
                              metadata={"source": rel, "label": label, "title": data.get("title", label)})
            docs.extend(splitter.split_documents([parent]))
    return docs

# 4) VectorStore ÎπåÎìú/Î°úÎìú
def build_vectorstore(batch=100) -> FAISS:
    docs = load_documents()
    print(f"üìÑ Ï¥ù {len(docs)}Í∞úÏùò Î¨∏ÏÑúÎ•º ÏûÑÎ≤†Îî©Ìï©ÎãàÎã§.")

    vs = None
    for i in tqdm(range(0, len(docs), batch), desc="üîÑ ÏûÑÎ≤†Îî© Ï§ë"):
        part = docs[i:i+batch]
        tmp = FAISS.from_texts(
            texts=[d.page_content for d in part],
            embedding=embedding_model,
            metadatas=[d.metadata for d in part]
        )
        if vs is None:
            vs = tmp
        else:
            vs.merge_from(tmp)
    vs.save_local("vectorstore_index")
    print("‚úÖ ÏûÑÎ≤†Îî© ÏôÑÎ£å Î∞è Ï†ÄÏû•Îê®: vectorstore_index/")
    return vs

def load_vectorstore() -> FAISS:
    if os.path.exists("vectorstore_index"):
        return FAISS.load_local("vectorstore_index",
                                embedding_model,
                                allow_dangerous_deserialization=True)
    return build_vectorstore()

# 5) Ïô∏Î∂Ä Í≥µÍ∞ú Í∞ùÏ≤¥
vectorstore = load_vectorstore()
